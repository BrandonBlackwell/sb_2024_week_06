{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03280e51-4d1d-478f-812c-d4b01b52630d",
   "metadata": {},
   "source": [
    "# Exercise 04: Hyperparameter Tuning Part 4: Automated Tuning\n",
    "\n",
    "**Make sure you've completed Exercises 01-03 before attempting this exercise!**\n",
    "\n",
    "In this exercise,\n",
    "you will explore techniques for automatically assessing hyperparameter settings.\n",
    "\n",
    "As usual, let's install packages and import modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6b16e-5159-4e44-8cd4-573106c56ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -U pip\n",
    "!{sys.executable} -m pip install -U scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5334b-abbe-47d9-bd1d-bfdd29cebc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import validation_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f784a9-540f-4e1d-93a6-aa6a9c5a259a",
   "metadata": {},
   "source": [
    "Once again we will use the model function and pipeline from Exercise 01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d17e7c4-0fa0-4bc4-9396-b1c70083fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"poly\",   PolynomialFeatures(include_bias=False)),\n",
    "    (\"linreg\", LinearRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f556197-482d-483c-80e0-a08ee28b9ef4",
   "metadata": {},
   "source": [
    "The following code\n",
    "generates the same samples\n",
    "that we used in Exercise 01.\n",
    "\n",
    "It then plots results for various combinations\n",
    "of scoring technique\n",
    "and number of cross-validation sets.\n",
    "\n",
    "Don't get too bogged down\n",
    "in the details of the scoring;\n",
    "all you really need to know\n",
    "is that higher is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ced815-5f8e-4750-8d33-cacc8f706d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "N_samples = 30\n",
    "degrees = np.arange(0,25)\n",
    "\n",
    "X_samples = np.sort(np.random.rand(N_samples))\n",
    "y_samples = true_fun(X_samples) + np.random.randn(N_samples) * 0.1\n",
    "\n",
    "cv_sizes = [3, 6, 10]\n",
    "scoring_infos = [\n",
    "    ['neg_root_mean_squared_error', [ -1.0, 0.10]],\n",
    "    ['explained_variance',          [ -2.0, 1.25]],\n",
    "    ['neg_mean_absolute_error',     [ -1.0, 0.10]],\n",
    "    ['r2',                          [ -0.6, 1.10]],\n",
    "]\n",
    "\n",
    "N_cols = len(cv_sizes)\n",
    "N_rows = len(scoring_infos)\n",
    "\n",
    "plt.figure(figsize=(5*N_cols, 5*N_rows))\n",
    "\n",
    "for row_idx in range(N_rows):\n",
    "    for col_idx in range(N_cols):\n",
    "            ax = plt.subplot(N_rows, N_cols, 1 + row_idx*N_cols + col_idx)\n",
    "\n",
    "            train_score, val_score = validation_curve(\n",
    "                pipeline,\n",
    "                X_samples[:,np.newaxis],\n",
    "                y_samples,\n",
    "                param_name='poly__degree',\n",
    "                param_range=degrees,\n",
    "                cv=cv_sizes[col_idx],\n",
    "                scoring=scoring_infos[row_idx][0],\n",
    "            )\n",
    "\n",
    "            plt.plot(degrees, np.median(train_score, 1), marker='o', color='blue', label='training score')\n",
    "            plt.plot(degrees, np.median(val_score, 1),   marker='o', color='red',  label='validation score')\n",
    "\n",
    "            plt.legend(loc='best')\n",
    "            plt.ylim(scoring_infos[row_idx][1][0], scoring_infos[row_idx][1][1])\n",
    "            plt.xlabel('degree')\n",
    "            plt.ylabel(scoring_infos[row_idx][0]);\n",
    "            plt.title('CV Size = {}'.format(cv_sizes[col_idx]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3400c58-86f6-4fb2-ac62-3023086d463c",
   "metadata": {},
   "source": [
    "**Why does the training score tend to rise and then level out?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c5a667-d22a-497d-80af-e089c8615c8e",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a829b8-5a4f-4ca9-b4d7-4d5938f693e1",
   "metadata": {},
   "source": [
    "**Why does the validation score tend to rise, level off, and then fall?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d7eb0-a67d-419a-8ca9-d596e9cc9f34",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc91cb6a-a50e-4a72-92b8-7facfb87c3da",
   "metadata": {},
   "source": [
    "**Based on the above plots, which polynomial degree(s) best models the data?  Justify your claim.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c71ad-b3f5-42d9-8cb9-4ab96a02f8a2",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fc662a-6d6c-4893-b6aa-a645037da1c9",
   "metadata": {},
   "source": [
    "**Based on the plots above, does cross-validation set size have a significant affect on hyperparameter tuning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d016a-164d-456d-8821-ace00879c40c",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e94c456-81a2-479d-a51b-3853ed5b989b",
   "metadata": {},
   "source": [
    "**Based on the plots above, does the scoring metric have a significant affect on hyperparameter tuning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6cee6e-cef9-4146-a067-0c605a5950dc",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac71f0-aaec-403f-b588-62df13a4869c",
   "metadata": {},
   "source": [
    "**The code for Exercises 01 and 02 contains a classic ML blunder (which you definitely noticed, right!?!).  What is it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a376701-4682-41e1-997d-4e3d5a0effd5",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafcf32f-8f0e-498e-91a0-d0d1e941b868",
   "metadata": {},
   "source": [
    "**What did you think of this week's exercises?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914dd58c-ab61-4ebd-9afa-2df3baac4787",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b1595-6d2c-47c7-ba22-d9a68f1cd68f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
