{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6353469c-af1a-45cf-a205-97db3793a14b",
   "metadata": {},
   "source": [
    "# Exercise 01: Hyperparameter Tuning Part 1: Warm-up\n",
    "\n",
    "In this exercise,\n",
    "you will explore how hyperparameter settings\n",
    "affect how ML models are fit to data samples.\n",
    "\n",
    "(Plots, etc., are heavily based on [this tutorial](https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html).)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ef161-749c-4ca1-8a5a-9bf048ca2970",
   "metadata": {},
   "source": [
    "Let's install the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1615778a-2de0-4d0f-92d5-3a67dbe9cb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -U pip\n",
    "!{sys.executable} -m pip install -U scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d87e55-5fb8-460c-aaa4-5d6f75a5c81c",
   "metadata": {},
   "source": [
    "and import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3715eb-b0eb-4ddf-96ab-f86dc44fc622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c84888-9dbd-4ba5-a800-9680ce917c6a",
   "metadata": {},
   "source": [
    "We will be attempting\n",
    "to use ML\n",
    "to learn an underlying model\n",
    "from noisy samples.\n",
    "\n",
    "The underlying model is defined by `true_func()` and will be plotted below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b6714-7316-4f89-a148-702a8f5adf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "xs = np.linspace(0, 1, 100)\n",
    "plt.plot(xs, true_fun(xs), label=\"True function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Underlying Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6956d0-df67-4b9e-acd2-80dedace6106",
   "metadata": {},
   "source": [
    "Let's compute some noisy samples and overlay them on the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a237d718-f076-4607-9095-9d4203f60899",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "\n",
    "N_samples = 30\n",
    "\n",
    "X_samples = np.sort(np.random.rand(N_samples))\n",
    "y_samples = true_fun(X_samples) + np.random.randn(N_samples) * 0.1\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "xs = np.linspace(0, 1, 1000)\n",
    "plt.plot(xs, true_fun(xs), label=\"True function\")\n",
    "plt.scatter(X_samples[:,np.newaxis], y_samples, color='m', s=30, label=\"Noisy Samples\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Noisy Samples\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd3379-aac4-41a2-ba89-47f991701a6d",
   "metadata": {},
   "source": [
    "Make sure you understand what you are looking at above!\n",
    "\n",
    "**If I want to make the purple dots more noisy, what change(s) should I make to the code above?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a57b4-a816-4bcd-827d-0c2020dacbfd",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6b7fa3-3e0d-44b1-8513-1d0e8e60539e",
   "metadata": {},
   "source": [
    "We are going to use ML\n",
    "to estimate the blue curve\n",
    "from the purple dots.\n",
    "\n",
    "The function we're trying to model isn't particularly sophisticated,\n",
    "so a polynomial should do the trick.\n",
    "\n",
    "The code below\n",
    "creates degree-2 polynomial features\n",
    "from our input points\n",
    "and then attempts to fit a degree-2 polynomial\n",
    "to those features\n",
    "using linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718308f8-a1da-4774-9e59-9d917532e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A pipeline is a canned set of operations to apply to the input data.\n",
    "## Once it manipulates the data, it acts like any other estimator.\n",
    "pipeline = Pipeline([\n",
    "    (\"poly\",   PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    (\"linreg\", LinearRegression()),\n",
    "])\n",
    "pipeline.fit(X_samples[:, np.newaxis], y_samples)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "xs = np.linspace(0, 1, 1000)\n",
    "plt.plot(xs, true_fun(xs), label=\"True function\")\n",
    "plt.scatter(X_samples[:,np.newaxis], y_samples, color=\"m\", s=30, label=\"Noisy Samples\")\n",
    "plt.plot(xs, pipeline.predict(xs[:, np.newaxis]), label=\"Model\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"Degree-2 Polynomial Fit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c77494-d870-4189-8fd9-efbf06785473",
   "metadata": {},
   "source": [
    "**Does this degree-2 polynomial model do a good of capturing the underlying function?  Justify your answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff538b8-479d-47dd-a173-5f0f72a18130",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b8a55c-b523-48ed-bb2f-cb5985b74133",
   "metadata": {},
   "source": [
    "Let's experiment with different polynomial degrees.\n",
    "\n",
    "The following code runs the same pipeline on the same input data,\n",
    "but adjusts the degree of the polynomial used as the ML model.\n",
    "It also gauges the model fit\n",
    "using mean squared error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26371ff6-6371-4940-b36f-daaeeb268699",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1, 2, 3, 4, 5, 6, 10, 15, 20]\n",
    "\n",
    "N_cols = 3\n",
    "N_rows = 1+(len(degrees)//N_cols)\n",
    "\n",
    "plt.figure(figsize=(5*N_cols, 5*N_rows))\n",
    "\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(N_rows, N_cols, i+1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"poly\",   PolynomialFeatures(degree=degrees[i], include_bias=False)),\n",
    "        (\"linreg\", LinearRegression()),\n",
    "    ])\n",
    "    pipeline.fit(X_samples[:, np.newaxis], y_samples)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(\n",
    "        pipeline,\n",
    "        X_samples[:, np.newaxis],\n",
    "        y_samples,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=10,\n",
    "    )\n",
    "\n",
    "    xs = np.linspace(0, 1, 1000)\n",
    "    plt.plot(xs, true_fun(xs), label=\"True function\")\n",
    "    plt.scatter(X_samples, y_samples, color=\"m\", s=30, label=\"Samples\")\n",
    "    plt.plot(xs, pipeline.predict(xs[:, np.newaxis]), label=\"Model\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "        degrees[i], -scores.mean(), scores.std()\n",
    "    ))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a7b91-68da-4a60-98fb-8d53ed0509a8",
   "metadata": {},
   "source": [
    "**Which degree polynomial do you think does the best job?  Why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5424ad3-d37b-420e-9a35-171c70543221",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cea5e1-f0f0-4655-b680-f0472db67788",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
